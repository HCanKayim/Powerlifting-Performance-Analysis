{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2db6c4e",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "This notebook contains the analysis from pca_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename='sample_openpowerlifting.csv'):\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    # Select numerical features only for this analysis\n",
    "    numerical_features = ['Age', 'BodyweightKg', 'WeightClassKg', 'Best3SquatKg', \n",
    "                         'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg', 'Wilks']\n",
    "    \n",
    "    # Create feature matrix X\n",
    "    X = data[numerical_features].copy()\n",
    "    \n",
    "    # Convert columns to numeric, replacing errors with NaN\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Create target variable y (Sex: M=0, F=1)\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(data['Sex'])\n",
    "    \n",
    "    # Handle any missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    return X, y, numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ccb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(X):\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    print(\"\\nExplained Variance Ratio for first 5 components:\")\n",
    "    for i in range(5):\n",
    "        print(f\"PC{i+1}: {explained_variance_ratio[i]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nCumulative Explained Variance Ratio for first 5 components: {cumulative_variance_ratio[4]:.4f}\")\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Explained Variance Ratio vs Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('pca_explained_variance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return X_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rf_with_pca(X_pca, y):\n",
    "    # Use first 5 principal components\n",
    "    X_pca_5 = X_pca[:, :5]\n",
    "    \n",
    "    # Initialize Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(rf, X_pca_5, y, cv=5)\n",
    "    \n",
    "    print(\"\\nRandom Forest Cross-validation results using first 5 Principal Components:\")\n",
    "    print(f\"Individual CV scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_contributions(pca, feature_names):\n",
    "    # Get the loadings (feature contributions) for first 5 PCs\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_[:5].T,\n",
    "        columns=[f'PC{i+1}' for i in range(5)],\n",
    "        index=feature_names\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(loadings, cmap='RdBu', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(5), [f'PC{i+1}' for i in range(5)])\n",
    "    plt.yticks(range(len(feature_names)), feature_names)\n",
    "    plt.title('Feature Contributions to First 5 Principal Components')\n",
    "    \n",
    "    # Add the values in the cells\n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(5):\n",
    "            plt.text(j, i, f'{loadings.iloc[i, j]:.2f}', \n",
    "                    ha='center', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_feature_contributions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9881d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    X, y, feature_names = load_data()\n",
    "    print(\"Original dataset shape:\", X.shape)\n",
    "    \n",
    "    # Perform PCA\n",
    "    X_pca, pca = perform_pca(X)\n",
    "    \n",
    "    # Evaluate Random Forest with PCA components\n",
    "    cv_scores = evaluate_rf_with_pca(X_pca, y)\n",
    "    \n",
    "    # Plot feature contributions\n",
    "    plot_feature_contributions(pca, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a256a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
